{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "46845004-4791-49d5-b23f-f673d34b080b",
   "metadata": {},
   "source": [
    "## Thesis Progress\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4a7bf00c",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "#### Current Worries\n",
    "\n",
    "- The **label frequencies** of the twemlab goldstandard training dataset. This is the distribution of the overall 1625 goldstandard emotion labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "820ba2a6",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets in goldstandard: 994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Christina\\AppData\\Local\\Temp\\ipykernel_12544\\2985161898.py:64: DtypeWarning: Columns (2,4,5,8,19,30) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  readfile = pd.read_csv('../Data/twemlab_goldstandards_original/boston_goldstandard.csv')\n"
     ]
    }
   ],
   "source": [
    "# Load TwEmLab Goldstandard for Birmingham\n",
    "tree1 = ET.parse('../Data/twemlab_goldstandards_original/birmingham_labels.xml')\n",
    "root1 = tree1.getroot()\n",
    "\n",
    "# check contents\n",
    "#root1[0][1].text\n",
    "\n",
    "# create dataframe from xml file\n",
    "data1 = []\n",
    "for tweet in root1.findall('Tweet'):\n",
    "    id = tweet.find('ID').text\n",
    "    label = tweet.find('Label').text\n",
    "    data1.append((id, label))\n",
    "\n",
    "df1 = pd.DataFrame(data1,columns=['id','label'])\n",
    " # df1.head()\n",
    "    \n",
    "# Load TwEmLab Birmingham Tweets\n",
    "tree2 = ET.parse('../Data/twemlab_goldstandards_original/birmingham_tweets.xml')\n",
    "root2 = tree2.getroot()\n",
    "\n",
    "# check contents\n",
    "# root2[0][1].text\n",
    "\n",
    "# create dataframe from xml file\n",
    "data2 = []\n",
    "for tweet in root2.findall('Tweet'):\n",
    "    id = tweet.find('ID').text\n",
    "    text = tweet.find('text').text\n",
    "    goldstandard = tweet.attrib.get(\"goldstandard\")\n",
    "    data2.append((id, text, goldstandard))\n",
    "\n",
    "df2 = pd.DataFrame(data2,columns=['id','text', 'goldstandard'])\n",
    "# df2.head()\n",
    "\n",
    " # merge the two separate dataframes based on id columns\n",
    "merge = pd.merge(df1, df2, on='id')\n",
    "\n",
    "# keep only the tweets that are part of the goldstandard\n",
    "twemlab = merge[merge['goldstandard'] == 'yes']\n",
    "print(f'Number of tweets in goldstandard: {len(twemlab)}')\n",
    "\n",
    "emotions = []\n",
    "# assign emotion label (happiness, anger, sadness, fear)\n",
    "for index, row in twemlab.iterrows():\n",
    "    if row['label'] == 'beauty' or row['label'] == 'happiness':\n",
    "        emotions.append('happiness')\n",
    "    elif row['label'] == 'anger/disgust':\n",
    "        emotions.append('anger')\n",
    "    elif row['label'] == 'sadness':\n",
    "        emotions.append('sadness')\n",
    "    elif row['label'] == 'fear':\n",
    "        emotions.append('fear')\n",
    "    else: \n",
    "        emotions.append('none')\n",
    "        \n",
    "twemlab['emotion'] = emotions\n",
    "\n",
    "twemlab_birmingham = twemlab[['id','text','emotion']]\n",
    "\n",
    "# check dataset\n",
    "# twemlab_birmingham.head(20)\n",
    "\n",
    "readfile = pd.read_csv('../Data/twemlab_goldstandards_original/boston_goldstandard.csv')\n",
    "twemlab_boston = readfile[['Tweet_ID', 'Tweet_timestamp', 'Tweet_text', 'Tweet_goldstandard_attribute', 'Tweet_longitude','Tweet_latitude','Tweet_timestamp','Emotion']]\n",
    "# use only rows that have text in them\n",
    "twemlab_boston = twemlab_boston[0:631]\n",
    "# twemlab_boston.head()\n",
    "\n",
    "emotions = []\n",
    "# assign emotion label (happiness, anger, sadness, fear)\n",
    "for index, row in twemlab_boston.iterrows():\n",
    "    if row['Emotion'] == 'beauty' or row['Emotion'] == 'happiness':\n",
    "        emotions.append('happiness')\n",
    "    elif row['Emotion'] == 'anger/disgust':\n",
    "        emotions.append('anger')\n",
    "    elif row['Emotion'] == 'sadness':\n",
    "        emotions.append('sadness')\n",
    "    elif row['Emotion'] == 'fear':\n",
    "        emotions.append('fear')\n",
    "    else: \n",
    "        emotions.append('none')\n",
    "        \n",
    "twemlab_boston['emotion'] = emotions\n",
    "\n",
    "twemlab_boston = twemlab_boston[['Tweet_ID','Tweet_text','emotion']]\n",
    "\n",
    "# check dataset\n",
    "# twemlab_boston.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "afc334a0",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Christina\\AppData\\Local\\Temp\\ipykernel_12544\\2854999063.py:5: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  emotions_in_twemlab_all = brim_emo.append(bost_emo, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# extract the emotion column from both dfs and merge\n",
    "brim_emo = twemlab_birmingham[['emotion']]\n",
    "bost_emo = twemlab_boston[['emotion']]\n",
    "\n",
    "emotions_in_twemlab_all = brim_emo.append(bost_emo, ignore_index=True)\n",
    "print(len(emotions_in_twemlab_all))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "2524bf02",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Value</th>\n",
       "      <th>Frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>none</td>\n",
       "      <td>916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>happiness</td>\n",
       "      <td>495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>anger</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sadness</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fear</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Value  Frequency\n",
       "0       none        916\n",
       "1  happiness        495\n",
       "2      anger        111\n",
       "3    sadness         73\n",
       "4       fear         30"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#value_counts = twemlab['label'].value_counts()\n",
    "value_counts = emotions_in_twemlab_all['emotion'].value_counts().reset_index()\n",
    "value_counts = value_counts.rename(columns={'index': 'Value', 'emotion': 'Frequency'})\n",
    "df_value_counts = pd.DataFrame(value_counts)\n",
    "\n",
    "dem_cols = df_value_counts[['Value', 'Frequency']]\n",
    "dem_cols"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b0d7c6de",
   "metadata": {},
   "source": [
    "- **Size and standard of training data**: The overall size of the training data for twemlab is 1625 and its aspect terms have been annotated by one individual \"ad hoc\" as per \"Annotating Twemlab Goldstandard Files to Include Aspect Term Labels\".</li>\n",
    "\n",
    "- **Mearsuring performance**: a robust testing dataset is needed. Above, I have shown a makeshift performance measure on the same dataset that the model was trained on. Drawing any meaningful conclusions based on this is precarious.\n",
    "\n",
    "- **Use different training data vs. annotate?** [SemEval2018 Task1 Affect in Tweets](https://competitions.codalab.org/competitions/17751#learn_the_details-datasets) (anger: 1700, fear: 2200, joy: 1600, sadness: 1500)\n",
    "\n",
    "\n",
    "|ID\t|Tweet\t|Affect Dimension\t|Intensity Score|\n",
    "|-----|-----|------|-----|\n",
    "|2017-En-10264\t|@xandraaa5 @amayaallyn6 shut up hashtags are cool #offended\t|anger|\t0.562|\n",
    "|2017-En-10072|\tit makes me so fucking irate jesus. nobody is calling ppl who like hajime abusive stop with the strawmen lmao\t|anger\t|0.750|\n",
    "|2017-En-11383\t|Lol Adam the Bull with his fake outrage...\t|anger\t|0.417|\n",
    "|2017-En-11102|\t@THATSSHAWTYLO passed away early this morning in a fast and furious styled car crash as he was leaving an ATL strip club. That's rough stuff|\tanger\t|0.354|\n",
    "|2017-En-20968|\t@RockSolidShow @Pat_Francis #revolting cocks if you think I'm sexy!|\tfear\t|0.292|\n",
    "|2017-En-21816|\t@Its_just_Huong I will beat you !!! Always thought id be gryffindor so this is a whole new world for me 😨😨😨 #excited #afraid\t|fear\t|0.667|\n",
    "|2017-En-40023|\tThis the most depressing shit ever|\tsadness\t|0.861|\n",
    "|2017-En-30793|\t@david_garrett Quite saddened.....no US dates, no joyous anticipation of attending a DG concert (since 2014). Happy you are keeping busy.\t|joy\t|0.140|\n",
    "\n",
    "- **Training capacities**: Batch size reduced due to out-of-memory errors. GRACE training is memory intensive (the authors use a nvidia tesla v100 gpu). Potential options: reduce float point precision? Currently having issues installing conda package for apex to do so. \n",
    "\n",
    "#### Current Questions\n",
    "\n",
    "- **Model optimisation**? The GRACE model uses GeLU (an \"advanced\" activation function), the standard BERT nn.embeddings layer, 12 transformer encoder layers and 2 decoder layers. On top of that it has two classification heads (both nn.Linear). During training the model uses additional functions for virtaul adversarial training and gradient harmonized loss calculation.\n",
    "\n",
    "- **Transfer Learning**? Arguments for: fine-tuning is much more accurate than feature-extraction. And the most efficient way of fine-tuning a model that will likely need to be fine-tuned again and again is transfer learning (adapters)\n",
    "\n",
    "#<img src=\"https://github.com/Christina1281995/demo-repo/blob/main/transfernlp2.JPG?raw=true\">\n",
    "<!-- <img src=\"https://github.com/Christina1281995/demo-repo/blob/main/transfernlp2.JPG?raw=true\" align=\"left\" width=\"49%\"> -->\n",
    "\n",
    "<img src=\"https://github.com/Christina1281995/demo-repo/blob/main/transfernlp.JPG?raw=true\">\n",
    "<!-- <img src=\"https://github.com/Christina1281995/demo-repo/blob/main/transfernlp.JPG?raw=true\" align=\"right\" width=\"49%\"> -->\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "- **Create pipline for entire workflow**? \n",
    "\n",
    "<img src=\"https://github.com/Christina1281995/demo-repo/blob/main/piplineworkflow.png?raw=true\" width=\"70%\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ff621122",
   "metadata": {},
   "source": [
    "#### Potential Approach to Thesis\n",
    "\n",
    "\n",
    "**“Pitch”**\n",
    "\n",
    "Aspect-based sentiment analysis (ABSA) has garnered attention in recent years due to its fine-grained approach to sentiment analysis. ABSA enables sentiment analysis at the entity or aspect level, instead of the entire document, resulting in better insights. ABSA has four sub-tasks, each identifying a different token-level piece of information. While numerous models have been developed for ABSA, there are only a handful of methods that have been applied to social media (Twitter) data for aspect term and sentiment extraction.\n",
    "\n",
    "However, current ABSA research has rarely explored the application of emotion detection for aspect-level sentiment analysis and it is not known that emotion detection has been applied to the End to End ABSA task. To date, there is also lacking research into the geographical distribution of emotions related to aspect terms. Additionally, there appears to be a lack of publicly available training data for aspect-level emotions on Twitter.\n",
    "\n",
    "Therefore, the thesis aims to generate a publicly accessible dataset for aspect-level emotions and investigate whether a model optimized for social media end-to-end ABSA can be trained on this more fine-grained data with comparable accuracy. The thesis will also explore whether this fine-grained approach, coupled with geographical analysis, reveals deeper and more varied insights into public opinion on specific aspect terms. The hypothesis is that a geographical analysis of aspect-based emotions provides a more nuanced geospatial view of the otherwise simplified \"negative\" and \"positive\" labels.\n",
    "\n",
    "<hr>\n",
    "\n",
    "<img src=\"https://github.com/Christina1281995/demo-repo/blob/main/Picture3.png?raw=true\" width=\"80%\">\n",
    "\n",
    "<hr>\n",
    "\n",
    "I. Introduction\n",
    "- Background and motivation for the research (need for fine-grained geographical, semantic and sentiment related information in real-world use cases such as COVID-19)\n",
    "- Research question, aims, and objectives\n",
    "- Hypothesis (a geographical analysis of aspect-based emotions provides a more nuanced view of the otherwise simplified \"negative\" and \"positive\" labels)\n",
    "- Outline of the thesis\n",
    "\n",
    "II. Literature Review\n",
    "- Overview of sentiment analysis and ABSA\n",
    "- Review of related work in aspect-level sentiment analysis\n",
    "- Review of related work in emotion detection for ABSA \n",
    "- Review of related work in geographical sentiment analysis\n",
    "\n",
    "III. Methodology\n",
    "- Description of the proposed methodology for generating a dataset for aspect-level emotions (labelling standard, data collection and preprocessing)\n",
    "- Description of the proposed methodology for training and testing the End to End ABSA model on the dataset (GRACE in some detail, performance metrics)\n",
    "- Description of the proposed methodology for performing geographical analysis on the aspect-based emotions (hot spot analysis –Getis-Ord Gi*)\n",
    "\n",
    "IV. Results\n",
    "- Description of the dataset and its properties (label distribution, count, stats of annotated twemlab goldstandard or SemEval 2018)\n",
    "- Evaluation of the End to End ABSA model on the dataset (performance metrics)\n",
    "- Analysis of the geographical distribution of aspect-based emotions (maps displaying aspect-term emotions)\n",
    "- Comparison with geographical results of other methodologies (maps displaying sentiments: doc-level sentiment analysis and original ABSA model)\n",
    "\n",
    "V. Discussion\n",
    "- Discussion of the results in relation to the research aims and hypothesis (does emotion ABSA show more detailed, nuanced view of the case study?) \n",
    "- Reflection on the limitations of the research (transferability, training capacities, …)\n",
    "\n",
    "VI. Conclusion and Future Work\n",
    "- Summary of the main findings\n",
    "- Suggestions for future research (model optimisation, transfer learning, pipelining the approach …)\n",
    "\n",
    "VII. References\n",
    "- List of cited works in the thesis\n",
    "\n",
    "VIII. Appendix\n",
    "- Description of the dataset and its format\n",
    "- Description of the End to End ABSA model (GRACE) and its parameters used for training etc.\n",
    "\n",
    "<hr>\n",
    "\n",
    "**Potential Research questions**\n",
    "\n",
    "1. Can a model optimised for social media end-to-end ABSA be trained on a more fine-grained set of emotions with comparable accuracy? \n",
    "2. How do the insights gained from analysing the emotions associated with aspect terms differ from those derived from traditional aspect-based sentiment analysis? (How can these insights be used to improve public opinion analysis?)\n",
    "3. What new insights can be gained from analysing the geographical distribution of aspect-based emotions, and how can this be used to inform decision-making processes?\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('GRACE_GPU')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "d330c58e3bb01c9a6aa22fdc5b13d0bc6cc6d676164106ca161027b468300435"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
